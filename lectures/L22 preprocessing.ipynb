{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "Today: A discussion of options during preprocessing to generate better predictions\n",
    "- Missing values\n",
    "- Outliers\n",
    "- Feature construction\n",
    "- Feature transformation/scaling\n",
    "- Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some say that applied ML is just data cleaning and processing\n",
    "\n",
    "- Remember the Nate Silver quote?\n",
    "- Andrew Ng, (Coursera, Stanford, Google Brain, Baidu... super influential CS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Missing Values\n",
    "\n",
    "- think about what the right replacement is!\n",
    "- why is it missing --> might help pick replacement or option\n",
    "- different variables should likely get different filling options\n",
    "- fancy (actually, easy) but can be powerful: NN replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outliers\n",
    "\n",
    "Can alter model estimations\n",
    "- change inference\n",
    "- reduce predictive power\n",
    "\n",
    "[Which of these 4 datasets has problems with outliers?](https://ledatascifi.github.io/ledatascifi-2021/content/03/04b-whyplot.html?highlight=outliers)\n",
    "\n",
    "[Options for dealing with them](https://ledatascifi.github.io/ledatascifi-2021/content/03/05d_outliers.html?highlight=outliers)\n",
    "- Drop \n",
    "- Transform\n",
    "    - winsorize: if a value is above p99, change to = p99\n",
    "    - scaling (next slide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In sklearn, some scalars & models are more sensitive to outliers...\n",
    "- Good idea to address by default!\n",
    "\n",
    "If you want to drop or winsorize inside of sklearn pipelines, you need to use [FunctionTransformer](https://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers) \n",
    "- This lets you use outside functions (like `pd`) inside pipelines\n",
    "- Still, this is harder than it should be, code-wise... should be updated in future\n",
    "\n",
    "Transforming within sklearn pipelines\n",
    "- Covered \n",
    "- `RobustScalar` works inside of pipelines and is robust to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature construction\n",
    "\n",
    "This is where really good models are made!\n",
    "\n",
    "Examples on the next slide!\n",
    "\n",
    "- interactions, e.g: \n",
    "    - story example: woman or child, woman and first class, finance AND coding\n",
    "- polynomial expansions. If you have $X_1$ and $X_2$:\n",
    "    - Poly of degree 2: $X_1$, $X_2$, $X1^2$, $X2^2$, $X_1*X_2$\n",
    "    - [Visual example](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#Validation-curves-in-Scikit-Learn)\n",
    "- binning: \n",
    "    - not profits as a #, but profit bins, e.g.: lowest, low, negative, zero, positive, high, highest\n",
    "    - remember the HW problem on on year vs year dummies?\n",
    "- extracting info from variables (e.g. date vars or text vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-104-790812230e62>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['sex'] = titanic['sex'] =='female'\n",
      "<ipython-input-104-790812230e62>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['sex'] = X.sex.astype(int)\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from statsmodels.formula.api import ols as sm_ols \n",
    "import statsmodels.api as sm        \n",
    "\n",
    "# for the polynomial\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_features= PolynomialFeatures(degree=3)\n",
    "\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "X = titanic[['age','sex','survived']]\n",
    "X['sex'] = titanic['sex'] =='female'\n",
    "X['sex'] = X.sex.astype(int)\n",
    "X = X.dropna()\n",
    "y = X['survived']\n",
    "X = X.drop('survived',axis=1)\n",
    "X = polynomial_features.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R2 of different models \n",
      "---------------------------\n",
      "Age + Gender:                 0.2911\n",
      "\n",
      "Child + Gender:               0.2994\n",
      "Child + Gender + Interaction: 0.3093\n",
      "\n",
      "poly deg 3 of (Age + Gender): 0.3166\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "R2 of different models \n",
    "---------------------------\n",
    "Age + Gender:                 {sm_ols('survived ~ age + sex',data=titanic).fit().rsquared.round(4)}\n",
    "\n",
    "Child + Gender:               {sm_ols('survived ~ (age<18) + sex',data=titanic).fit().rsquared.round(4)}\n",
    "Child + Gender + Interaction: {sm_ols('survived ~ (age<18) + sex + sex*(age<18)',data=titanic).fit().rsquared.round(4)}\n",
    "\n",
    "poly deg 3 of (Age + Gender): {sm.OLS(y,X).fit().rsquared.round(4)}\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scaling and transformation\n",
    "\n",
    "https://youtu.be/9rBc3rTsJsY?t=195\n",
    "- StandardScalar is sensitive to outliers (which change mean & std)\n",
    "- RobustScalar uses percentiles (not sensitive to outliers)\n",
    "- [Skewed variables?](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_map_data_to_normal.html#sphx-glr-auto-examples-preprocessing-plot-map-data-to-normal-py) Use PowerTransformer, possibly\n",
    "\n",
    "Illustration: SVM with 2 versions\n",
    "- with and without scaling where one var is x10000.\n",
    "- will post example later today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Feature selection](https://scikit-learn.org/stable/modules/feature_selection.html) and/or extraction\n",
    "\n",
    "- If you have too many variables, you will create an overfit model!\n",
    "    - The link above shows options in sklearn to pick some variables\n",
    "    - Options: `rfecv`, `selectfrommodel`, `selectKBest`, `Lasso` (HW!)\n",
    "    - Also: Picking based on \"feature importance\" from bagging/tree models \n",
    "\n",
    "An alternative to picking variables is \"combining them\" via PCA\n",
    "- You have LOTS of vars AND think the \"true\" number that matters is low\n",
    "- [illustration](https://www.textbook.ds100.org/ch/25/pca_dims.html#intuition-for-reducing-dimensions)\n",
    "- pros: reduces overfitting, quicker estimation\n",
    "- cons: hard (very!) to interpret what the model is doing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next week\n",
    "\n",
    "- Continue the discussion of preprocessing with practice\n",
    "- Modeling tips (things that win competitions and endear bosses)\n",
    "- Discussion of projects\n",
    "\n",
    "### Student demos \n",
    "- Please send an email if you want a change to do a demo for participation (I know 6-8 of you haven't yet from my records)\n",
    "\n",
    "![](https://media.giphy.com/media/H7x1H0veAJlo4/giphy.gif)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
